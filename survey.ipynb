{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"abstract\" id=\"org6820380\">\n<p>\nRobot@Home is a dataset of raw data captured from the sensors of a mobile robot\nin different indoor navigation sessions carried out in different homes. The\ndataset is suitable for the development and testing of indoor mobile robotics\nalgorithms, especially in the field of semantic mapping. In this paper we\npresent Robot@Home2 Toolbox consisting of a Python package, a database and a set\nof learning resources that comes to substitute original dataset for easy and\nusable experimentation. The Python package contains database interface functions\nand display functions, among others. The database forms the dataset itself\naccording to a relational model. In addition, the toolbox contains training\nresources in the form of Jupyter notebooks. Robot@Home2 is open source and it is\navailable from the corresponding git repository or by installing it as a\nstandard Python package. The database can be easily downloaded programatically\nor by accessing a public repository as Zenodo. In addition, the installation and\nexecution of Robot@Home2 is possible both in the local environment and in the\ncloud.\n</p>\n\n</div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix A. A survey on the previous use of Robot@Home dataset\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|---|---|---|---|---|---|---|---|\n| paper|year|main topics|A|B|C|D|E|\n|---|---|---|---|---|---|---|---|\n| [a href=\"&amp;ruiz-sarmiento2016_probabilistic_techniques\">&amp;ruiz-sarmiento2016_probabilistic_techniques</a](a href=\"&amp;ruiz-sarmiento2016_probabilistic_techniques\">&amp;ruiz-sarmiento2016_probabilistic_techniques</a)|2016|semantic mapping|||||&bull;|\n| [a href=\"&amp;ruiz-sarmiento2017_building_multiversal\">&amp;ruiz-sarmiento2017_building_multiversal</a](a href=\"&amp;ruiz-sarmiento2017_building_multiversal\">&amp;ruiz-sarmiento2017_building_multiversal</a)|2017|semantic mapping|||||&bull;|\n| [a href=\"&amp;nakamura2017_ensemble\">&amp;nakamura2017_ensemble</a](a href=\"&amp;nakamura2017_ensemble\">&amp;nakamura2017_ensemble</a)|2017|object categorization, dataset|||&bull;|||\n| [a href=\"&amp;ruiz-sarmiento2017_modelado_contexto\">&amp;ruiz-sarmiento2017_modelado_contexto</a](a href=\"&amp;ruiz-sarmiento2017_modelado_contexto\">&amp;ruiz-sarmiento2017_modelado_contexto</a)|2017|semantic mapping|||||&bull;|\n| [a href=\"&amp;tarifa2017_motion\">&amp;tarifa2017_motion</a](a href=\"&amp;tarifa2017_motion\">&amp;tarifa2017_motion</a)|2017|planar odometry|&bull;|||||\n| [a href=\"&amp;jaimez2017_motion_estimation\">&amp;jaimez2017_motion_estimation</a](a href=\"&amp;jaimez2017_motion_estimation\">&amp;jaimez2017_motion_estimation</a)|2017|planar odometry|&bull;||||&bull;|\n| [a href=\"&amp;deeken2018_grounding\">&amp;deeken2018_grounding</a](a href=\"&amp;deeken2018_grounding\">&amp;deeken2018_grounding</a)|2018|semantic mapping||||&bull;||\n| [a href=\"&amp;jaimez2018_robust\">&amp;jaimez2018_robust</a](a href=\"&amp;jaimez2018_robust\">&amp;jaimez2018_robust</a)|2018|plannar odometry|||||&bull;|\n| [a href=\"&amp;gunther2018_context\">&amp;gunther2018_context</a](a href=\"&amp;gunther2018_context\">&amp;gunther2018_context</a)|2018|3D world modelling||||&bull;||\n| [a href=\"&amp;roa-borbolla2018_realistic\">&amp;roa-borbolla2018_realistic</a](a href=\"&amp;roa-borbolla2018_realistic\">&amp;roa-borbolla2018_realistic</a)|2018|map generation, simulation||||&bull;||\n| [a href=\"&amp;ferri2018_computing_fast\">&amp;ferri2018_computing_fast</a](a href=\"&amp;ferri2018_computing_fast\">&amp;ferri2018_computing_fast</a)|2018|motion planning|&bull;||&bull;|||\n| [a href=\"&amp;suh2018_semantic_task\">&amp;suh2018_semantic_task</a](a href=\"&amp;suh2018_semantic_task\">&amp;suh2018_semantic_task</a)|2018|task planning||||&bull;||\n| [a href=\"&amp;balloch2018_unbiasing\">&amp;balloch2018_unbiasing</a](a href=\"&amp;balloch2018_unbiasing\">&amp;balloch2018_unbiasing</a)|2018|semantic segmentation||||&bull;|&bull;|\n| [a href=\"&amp;pire2019_rosario\">&amp;pire2019_rosario</a](a href=\"&amp;pire2019_rosario\">&amp;pire2019_rosario</a)|2019|SLAM, dataset||||&bull;||\n| [a href=\"&amp;ruiz-sarmiento2019_ontology\">&amp;ruiz-sarmiento2019_ontology</a](a href=\"&amp;ruiz-sarmiento2019_ontology\">&amp;ruiz-sarmiento2019_ontology</a)|2019|object recognition|||||&bull;|\n| [a href=\"&amp;zuniga2019_automatic\">&amp;zuniga2019_automatic</a](a href=\"&amp;zuniga2019_automatic\">&amp;zuniga2019_automatic</a)|2019|sensor calibration|||&bull;|||\n| [a href=\"&amp;chaves2019_integration\">&amp;chaves2019_integration</a](a href=\"&amp;chaves2019_integration\">&amp;chaves2019_integration</a)|2019|semantic mapping|||||&bull;|\n| [a href=\"&amp;martinez2019_fukuoka\">&amp;martinez2019_fukuoka</a](a href=\"&amp;martinez2019_fukuoka\">&amp;martinez2019_fukuoka</a)|2019|dataset||||&bull;||\n| [a href=\"&amp;delapuente2019_robot\">&amp;delapuente2019_robot</a](a href=\"&amp;delapuente2019_robot\">&amp;delapuente2019_robot</a)|2019|indoor navigation||||&bull;||\n| [a href=\"&amp;mishra2019_ego\">&amp;mishra2019_ego</a](a href=\"&amp;mishra2019_ego\">&amp;mishra2019_ego</a)|2019|robotic hardware|||&bull;|||\n| [a href=\"&amp;zuniga-noel2019_intrinsic\">&amp;zuniga-noel2019_intrinsic</a](a href=\"&amp;zuniga-noel2019_intrinsic\">&amp;zuniga-noel2019_intrinsic</a)|2019|sensor calibration|||&bull;|||\n| [a href=\"&amp;monroy2019_olfaction_vision\">&amp;monroy2019_olfaction_vision</a](a href=\"&amp;monroy2019_olfaction_vision\">&amp;monroy2019_olfaction_vision</a)|2019|robotic olfaction, obj. rec.|||||&bull;|\n| [a href=\"&amp;ruiz-sarmiento2019_tutorial_on\">&amp;ruiz-sarmiento2019_tutorial_on</a](a href=\"&amp;ruiz-sarmiento2019_tutorial_on\">&amp;ruiz-sarmiento2019_tutorial_on</a)|2019|training|||||&bull;|\n| [a href=\"&amp;fabro2019_design_development\">&amp;fabro2019_design_development</a](a href=\"&amp;fabro2019_design_development\">&amp;fabro2019_design_development</a)|2019|dataset|||&bull;|||\n| [a href=\"&amp;baltanas2019_coleccion_jupyter\">&amp;baltanas2019_coleccion_jupyter</a](a href=\"&amp;baltanas2019_coleccion_jupyter\">&amp;baltanas2019_coleccion_jupyter</a)|2019|training|||&bull;|||\n| [a href=\"&amp;moreno2020_automatic\">&amp;moreno2020_automatic</a](a href=\"&amp;moreno2020_automatic\">&amp;moreno2020_automatic</a)|2020|path planning|||||&bull;|\n| [a href=\"&amp;chen2020_advanced\">&amp;chen2020_advanced</a](a href=\"&amp;chen2020_advanced\">&amp;chen2020_advanced</a)|2020|dataset||&bull;||&bull;||\n| [a href=\"&amp;fernandez-chaves2020_from_object\">&amp;fernandez-chaves2020_from_object</a](a href=\"&amp;fernandez-chaves2020_from_object\">&amp;fernandez-chaves2020_from_object</a)|2020|reasoning, categorization|||||&bull;|\n| [a href=\"&amp;qi2020_object_lidar\">&amp;qi2020_object_lidar</a](a href=\"&amp;qi2020_object_lidar\">&amp;qi2020_object_lidar</a)|2020|semantic mapping|||||&bull;|\n| [a href=\"&amp;andersone2020_quality_evaluation\">&amp;andersone2020_quality_evaluation</a](a href=\"&amp;andersone2020_quality_evaluation\">&amp;andersone2020_quality_evaluation</a)|2020|map merging|||||&bull;|\n| [a href=\"&amp;li2020_relative_pose\">&amp;li2020_relative_pose</a](a href=\"&amp;li2020_relative_pose\">&amp;li2020_relative_pose</a)|2020|pose estimation|||||&bull;|\n| [a href=\"&amp;ruiz-sarmiento2020_tutorial_python\">&amp;ruiz-sarmiento2020_tutorial_python</a](a href=\"&amp;ruiz-sarmiento2020_tutorial_python\">&amp;ruiz-sarmiento2020_tutorial_python</a)|2020|training|||||&bull;|\n| [a href=\"&amp;maffei2020_global_localization\">&amp;maffei2020_global_localization</a](a href=\"&amp;maffei2020_global_localization\">&amp;maffei2020_global_localization</a)|2020|localization, path planning|||||&bull;|\n| [a href=\"&amp;garg2020_semantics_robotic\">&amp;garg2020_semantics_robotic</a](a href=\"&amp;garg2020_semantics_robotic\">&amp;garg2020_semantics_robotic</a)|2020|semantic mapping, survey||&bull;||&bull;||\n| [a href=\"&amp;roa-borbolla2020_algorithm_comparison\">&amp;roa-borbolla2020_algorithm_comparison</a](a href=\"&amp;roa-borbolla2020_algorithm_comparison\">&amp;roa-borbolla2020_algorithm_comparison</a)|2020|path planning||||&bull;||\n| [a href=\"&amp;othman2020_towards\">&amp;othman2020_towards</a](a href=\"&amp;othman2020_towards\">&amp;othman2020_towards</a)|2020|indoor navigation|&bull;|||&bull;||\n| [a href=\"&amp;burgueno2020_collection_of\">&amp;burgueno2020_collection_of</a](a href=\"&amp;burgueno2020_collection_of\">&amp;burgueno2020_collection_of</a)|2020|training||||&bull;||\n| [a href=\"&amp;pierre2020_localisation\">&amp;pierre2020_localisation</a](a href=\"&amp;pierre2020_localisation\">&amp;pierre2020_localisation</a)|2020|localization|&bull;||&bull;|||\n| [a href=\"&amp;shu2021_slam_field\">&amp;shu2021_slam_field</a](a href=\"&amp;shu2021_slam_field\">&amp;shu2021_slam_field</a)|2021|SLAM|||&bull;|||\n| [a href=\"&amp;yu2021_drsnet\">&amp;yu2021_drsnet</a](a href=\"&amp;yu2021_drsnet\">&amp;yu2021_drsnet</a)|2021|categorization|||&bull;|||\n| [a href=\"&amp;fernandez-chaves2021_vimantic\">&amp;fernandez-chaves2021_vimantic</a](a href=\"&amp;fernandez-chaves2021_vimantic\">&amp;fernandez-chaves2021_vimantic</a)|2021|semantic mapping|||||&bull;|\n| [a href=\"&amp;burgueno-romero2021_autonomous\">&amp;burgueno-romero2021_autonomous</a](a href=\"&amp;burgueno-romero2021_autonomous\">&amp;burgueno-romero2021_autonomous</a)|2021|path planning, learning|||&bull;|||\n| [a href=\"&amp;suveges2021_egomap\">&amp;suveges2021_egomap</a](a href=\"&amp;suveges2021_egomap\">&amp;suveges2021_egomap</a)|2021|SLAM, dataset|||&bull;|||\n| [a href=\"&amp;li2021_belief_space\">&amp;li2021_belief_space</a](a href=\"&amp;li2021_belief_space\">&amp;li2021_belief_space</a)|2021|navigation, uncertainty|&bull;|||||\n| [a href=\"&amp;asmanis2021_combining_semantics\">&amp;asmanis2021_combining_semantics</a](a href=\"&amp;asmanis2021_combining_semantics\">&amp;asmanis2021_combining_semantics</a)|2021|SLAM|&bull;||&bull;|||\n| [a href=\"&amp;qu2021_outline_multi\">&amp;qu2021_outline_multi</a](a href=\"&amp;qu2021_outline_multi\">&amp;qu2021_outline_multi</a)|2021|SLAM, sensor fusion|||||&bull;|\n| [a href=\"&amp;salhi2021_intelligent_embedded\">&amp;salhi2021_intelligent_embedded</a](a href=\"&amp;salhi2021_intelligent_embedded\">&amp;salhi2021_intelligent_embedded</a)|2021|SLAM|&bull;||&bull;|||\n| [a href=\"&amp;jin2021_semantic_mapping\">&amp;jin2021_semantic_mapping</a](a href=\"&amp;jin2021_semantic_mapping\">&amp;jin2021_semantic_mapping</a)|2021|semantic mapping|||||&bull;|\n| [a href=\"&amp;chamzas2021_motionbenchmaker\">&amp;chamzas2021_motionbenchmaker</a](a href=\"&amp;chamzas2021_motionbenchmaker\">&amp;chamzas2021_motionbenchmaker</a)|2021|dataset||&bull;|&bull;|||\n| [a href=\"&amp;ruiz-sarmiento2021_jupyter_notebooks\">&amp;ruiz-sarmiento2021_jupyter_notebooks</a](a href=\"&amp;ruiz-sarmiento2021_jupyter_notebooks\">&amp;ruiz-sarmiento2021_jupyter_notebooks</a)|2021|training|||&bull;|||\n| [a href=\"&amp;luperto2021_exploration_indoor\">&amp;luperto2021_exploration_indoor</a](a href=\"&amp;luperto2021_exploration_indoor\">&amp;luperto2021_exploration_indoor</a)|2021|navigation, uncertainty|||||&bull;|\n| [a href=\"&amp;matez-bandera2021_efficient\">&amp;matez-bandera2021_efficient</a](a href=\"&amp;matez-bandera2021_efficient\">&amp;matez-bandera2021_efficient</a)|2021|categorization|||||&bull;|\n| [a href=\"&amp;setiono2021_novel_room\">&amp;setiono2021_novel_room</a](a href=\"&amp;setiono2021_novel_room\">&amp;setiono2021_novel_room</a)|2021|categorization|||||&bull;|\n| [a href=\"&amp;ge2021_capacitive_piezoresistive\">&amp;ge2021_capacitive_piezoresistive</a](a href=\"&amp;ge2021_capacitive_piezoresistive\">&amp;ge2021_capacitive_piezoresistive</a)|2021|sensor hardware|||&bull;|||\n| [a href=\"&amp;liu2021_simultaneous_localization\">&amp;liu2021_simultaneous_localization</a](a href=\"&amp;liu2021_simultaneous_localization\">&amp;liu2021_simultaneous_localization</a)|2021|SLAM, dataset, survey||&bull;|&bull;|||\n| [a href=\"&amp;liu2021_datasets_evaluation\">&amp;liu2021_datasets_evaluation</a](a href=\"&amp;liu2021_datasets_evaluation\">&amp;liu2021_datasets_evaluation</a)|2021|SLAM, dataset, survey||&bull;|&bull;|||\n|---|---|---|---|---|---|---|---|\n\nSince its publication, Robot@Home dataset\n[&ruiz-sarmiento2017_robotic_dataset>](&ruiz-sarmiento2017_robotic_dataset>)has been referenced in a significant\nnumber of papers. We have carried out a survey (Table [1](#orgb3966f9)) to understand how the community\nhas been using the dataset and the challenges they have faced using it.\n\nOn some occasions the dataset was referenced as an example of the importance of datasets\nin the area of mobile robotics\n[&nakamura2017_ensemble;&zuniga2019_automatic;&mishra2019_ego;&zuniga-noel2019_intrinsic;&fabro2019_design_development;&shu2021_slam_field;&yu2021_drsnet;&burgueno-romero2021_autonomous;&suveges2021_egomap;&chamzas2021_motionbenchmaker;&ge2021_capacitive_piezoresistive;&liu2021_simultaneous_localization;&liu2021_datasets_evaluation](&nakamura2017_ensemble;&zuniga2019_automatic;&mishra2019_ego;&zuniga-noel2019_intrinsic;&fabro2019_design_development;&shu2021_slam_field;&yu2021_drsnet;&burgueno-romero2021_autonomous;&suveges2021_egomap;&chamzas2021_motionbenchmaker;&ge2021_capacitive_piezoresistive;&liu2021_simultaneous_localization;&liu2021_datasets_evaluation),\nand on other occasions more specifically as a dataset oriented to semantic\nmapping\n[&deeken2018_grounding;&gunther2018_context;&roa-borbolla2018_realistic;&suh2018_semantic_task;&pire2019_rosario;&martinez2019_fukuoka;&delapuente2019_robot;&garg2020_semantics_robotic;&roa-borbolla2020_algorithm_comparison;&burgueno2020_collection_of](&deeken2018_grounding;&gunther2018_context;&roa-borbolla2018_realistic;&suh2018_semantic_task;&pire2019_rosario;&martinez2019_fukuoka;&delapuente2019_robot;&garg2020_semantics_robotic;&roa-borbolla2020_algorithm_comparison;&burgueno2020_collection_of).\nIt has also served as a source of inspiration for PhD thesis\n[&ruiz-sarmiento2016_probabilistic_techniques;&tarifa2017_motion;&jaimez2017_motion_estimation;&ferri2018_computing_fast;&othman2020_towards;&pierre2020_localisation;&li2021_belief_space;&asmanis2021_combining_semantics;&salhi2021_intelligent_embedded](&ruiz-sarmiento2016_probabilistic_techniques;&tarifa2017_motion;&jaimez2017_motion_estimation;&ferri2018_computing_fast;&othman2020_towards;&pierre2020_localisation;&li2021_belief_space;&asmanis2021_combining_semantics;&salhi2021_intelligent_embedded)\nand as an education resource [&ruiz-sarmiento2021_jupyter_notebooks;&ruiz-sarmiento2019_tutorial_on;&ruiz-sarmiento2020_tutorial_python](&ruiz-sarmiento2021_jupyter_notebooks;&ruiz-sarmiento2019_tutorial_on;&ruiz-sarmiento2020_tutorial_python)\n\nHowever, where the data set has been most useful and of greatest interest to us\nhas been in those works in which it has been used for the purpose for which it\nwas created, that is, as a testing platform for the development of algorithms.\n\nRobot@Home dataset has been exploited for a variety of tasks. Starting with\nsemantic mapping, in\n[&ruiz-sarmiento2017_building_multiversal;&monroy2019_olfaction_vision](&ruiz-sarmiento2017_building_multiversal;&monroy2019_olfaction_vision)\nRobot@Home dataset was used to check the suitability of a probabilistic\nrepresentation in form of semantic map and its capacity to handle uncertain\ninformation. This map is an extension of traditional semantic maps for robotics,\nwith the ability to coherently manage uncertain information coming from, for\nexample, object recognition or gas classification processes, and reference them\nto the location where they were acquired into a metric map. Additionally, it\nalso comprises semantic information codified by means of an ontology, enabling\nthe execution of high-level reasoning tasks. [&chaves2019_integration](&chaves2019_integration)\nproposes the integration of a cnn into a robotic architecture to build semantic\nmaps of indoor environments and carries out experiments with Robot@Home dataset.\nOn the other hand, [&fernandez-chaves2021_vimantic>](&fernandez-chaves2021_vimantic>)presents ViMantic as a\nnovel semantic mapping architecture for the building and maintenance of such\nmaps. Experiments were carried out with the Robot@Home dataset considering\nmultiple robots collecting data from the same environment, hence enabling the\ntesting of multi-agent scenarios. In [&jin2021_semantic_mapping>](&jin2021_semantic_mapping>)a new deep\nlearning-based image feature fusion method is presented. The RGB feature\ninformation extracted by a classification network and a detection network are\nintegrated to improve the robot’s scene recognition ability and make the\nacquired semantic information more accurate. Robot@Home dataset is used to test\na 2d metric map obtained with the proposed the method.\n\nOther works related to room categorization have made use of the dataset. In\n[&fernandez-chaves2020_from_object>](&fernandez-chaves2020_from_object>)proposes a room categorization system\nbased on a Bayesian probabilistic framework that combines object detections and\nits semantics. The proposed system is evaluated in houses from the Robot@Home\ndataset, validating its effectiveness under real-world conditions. Moreover,\n[&setiono2021_novel_room>](&setiono2021_novel_room>)implements room categorization via scene\nunderstanding by integrating available object information in the scene, proposing\na novel approach based on the prior knowledge of the object appearance frequency\nin the specific room category inside the house. The proposed approach is tested\nand evaluated by applying the Robot@Home dataset using the available RGB images\nunder specific room categories.\n\nIn [&tarifa2017_motion;&jaimez2018_robust>](&tarifa2017_motion;&jaimez2018_robust>)simulations using Robot@Home and\nother datasets are carried out to address the estimation of 2D and 3D motion\nwith different kinds of range sensors. Otherwise, ontology-based conditional\nrandom fields address the problem of object recognition\n[&ruiz-sarmiento2019_ontology>](&ruiz-sarmiento2019_ontology>)using again scenes from Robot@Home to test the\napproach. [&moreno2020_automatic>](&moreno2020_automatic>)introduces an automatic waypoint generation\nmethod to improve robot navigation through narrow spaces. This work mainly uses\nRobot@Home dataset and justifies it due to the lack of publicly available\ndatabases that contain occupancy grids of real houses, being usually focused on\nlabs and offices instead. [&balloch2018_unbiasing>](&balloch2018_unbiasing>)proposes improving the\nperformance of real-time segmentation frameworks on robot perception data by\ntransferring features learned from synthetic segmentation data. Their work takes\nadvantage of Robot@Home by fine-tuning on various subsets of the dataset to\nquantify the benefits as the amount of supervised fine-tuning data is decreased.\nMoreover [&luperto2020_exploration;&luperto2021_exploration_indoor>](&luperto2020_exploration;&luperto2021_exploration_indoor>)present\nan approach to map building that exploits a prediction of the geometric\nstructure of the unknown parts of an environment to improve exploration\nperformance applying it to partial grid maps acquired in real environments. An\nexample of a computed map obtained by a real robot is created from the\nRobot@Home dataset.\n\nIn [&qi2020_object_lidar>](&qi2020_object_lidar>)an object semantic grid mapping system with 2D\nLiDAR and RGB-D sensors is proposed to solve the lack of semantic information to\nendow the robots with the ability of social goal selection and human-friendly\noperation modes. To verify the the effectiveness of the system the Robot@Home\ndataset is used again. On the other hand [&andersone2020_quality_evaluation](&andersone2020_quality_evaluation)\nproposes a method that allows the quality evaluation of occupancy grid maps\nwithout the need for ground truth maps. The method uses Convolutional Neural\nNetwork (CNN) for map fragment classification, for map quality evaluation as\nwell as for evaluation of map regions. To train and test the CNN, data of\nvarious quality maps was collected from several open source data sets including\nRobot@Home.\n\nFurthermore, [&li2020_relative_pose>](&li2020_relative_pose>)presents a complete comprehensive study\nof the relative pose estimation problem for a calibrated camera constrained by\nknown SE(3) invariant. To compare some approaches RGBD images from front and\nleft cameras of Robot@Home are used. [&maffei2020_global_localization](&maffei2020_global_localization)\npresents a localization strategy using floor plan as map, which is based on\nspatial density information computed from dense depth data of RGB-D cameras. The\nexperimental validation is made using the Robot@home dataset. As the ground\ntruth of the robot pose and the odometry are not directly available, they are\nrespectively generated using SLAM and scan matching techniques. For each\nscenario, two types of tests were performed: multi-camera by using the four\nRGB-D cameras, and single-camera by using only the RGB-D camera facing forward.\nAnd [&matez-bandera2021_efficient>](&matez-bandera2021_efficient>)presents an attention mechanism for mobile\nrobots to face the problem of place categorization. Robot@Home is used to\ndemostrate that the proposal generalizes well for the two main paradigms of\nplace categorization (object-based and image-based), outperforming typical\ncamera-configurations (fixed and continuously-rotating) and a pure-exploratory\napproach, both in quickness and accuracy. As Robot@Home does not explicitly\noffer a controllable pan unit a virtual one with degrees of pan motion and a\nmaximum rotation speed of degrees per second is generated by interpolating the\nview from the four available fixed cameras.\n\n[PhD.bib](PhD.bib)\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}